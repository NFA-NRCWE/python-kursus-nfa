{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be76b02f",
   "metadata": {},
   "source": [
    "### Installing and using Ollama locally with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070ff0d",
   "metadata": {},
   "source": [
    "- Open windows powershell -> type \"winget install --id Ollama.Ollama\"\n",
    "- Once complete, open anaconda powershell (or restart windows powershell) and type \"ollama pull mistral\"\n",
    "- (Optional): \n",
    "    - Create new environment (still in anaconda powershell): type \"conda create -n ollama_env python\"\n",
    "    -   Once complete, type \"conda activate ollama_env\"\n",
    "- Open your editor of choice and make sure your new environment is active (if you are reading this in a jupyter notebook, make sure to switch to your ollama environment, if you have made one)\n",
    "- Try to run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f33e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from httpx>=0.27->ollama) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\b061621\\appdata\\local\\anaconda3\\envs\\ollama_env\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c0d4e",
   "metadata": {},
   "source": [
    "Import the ollama package (https://github.com/ollama/ollama-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c70e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43809cfb",
   "metadata": {},
   "source": [
    "#### Examples of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d55dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's a simple Python function to check if a number is prime:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    elif n <= 3:\n",
      "        return True\n",
      "    elif n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "```\n",
      "\n",
      "This function works by first checking if the number is less than or equal to 1, in which case it's not prime. Then it checks for small factors like 2 and 3. After that, it skips every other number starting from 5 because every even number greater than 2 can be written as the sum of two primes (e.g., 6 = 3 + 3).\n",
      "\n",
      "The function then starts checking for divisors by incrementing `i` in steps of 6 since all numbers greater than 5 that are not of the form 4*k+1 or 4*k+3 have a prime factor less than or equal to âˆšn and can be found using this pattern (e.g., 7 is the next prime after 5, 11 is the next prime after 7, and so on).\n"
     ]
    }
   ],
   "source": [
    "code_prompt = \"Write a Python function that checks if a number is prime.\"\n",
    "response = ollama.generate(model='mistral', prompt=code_prompt)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf091be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      " The sky appears yellow, orange, or red during sunrise and sunset due to a phenomenon called Rayleigh scattering. During these times, sunlight has to pass through more of Earth's atmosphere, and the shorter wavelength blue light is scattered out in all directions while the longer wavelength red, orange, and yellow light are able to travel further towards the observer.\n",
      "\n",
      "However, if you're asking about a specific instance where the sky appears yellow (not during sunrise or sunset), it could be due to various factors like pollution, dust storms, or even certain weather conditions such as fog or haze. These particles in the air scatter sunlight and can give the sky a yellowish tint.\n"
     ]
    }
   ],
   "source": [
    "print(\"Streaming response:\")\n",
    "for chunk in ollama.generate('mistral', 'Why is the sky yellow?', stream=True):\n",
    "    print(chunk['response'], end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba181e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ahoy matey! Me ship be a fine vessel, aye. A swift schooner named the Sea Dog's Delight, she be equipped with sturdy sails an' cannonballs aplenty. She sails through seas rough and calm, always ready for adventure on the high seas. Arr!\n"
     ]
    }
   ],
   "source": [
    "# Define a system prompt\n",
    "system_prompt = \"You speaks and sounds like a pirate with short sentences.\"\n",
    "# Chat with a system prompt\n",
    "response = ollama.chat('mistral', \n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': system_prompt},\n",
    "                    {'role': 'user', 'content': 'Tell me about your boat.'}\n",
    "                ])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8c6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  OpenAI has released Ollama, a tool that allows users to run large language models on their local machines for enhanced privacy, control, efficiency, and cost reduction by utilizing multiple open-source models via a user-friendly interface.\n"
     ]
    }
   ],
   "source": [
    "# Example: Summarize a paragraph of text\n",
    "text = \"\"\"\n",
    "OpenAI has introduced a new tool called Ollama that lets users run large language models on local machines.\n",
    "This approach emphasizes privacy and control, as data does not leave the user's environment.\n",
    "Developers can leverage various open-source models through a simple interface, improving efficiency and reducing costs.\n",
    "\"\"\"\n",
    "prompt = f\"Summarize the following text in one sentence:\\n\\\"\\\"\\\"\\n{text}\\n\\\"\\\"\\\"\"\n",
    "result = ollama.generate(model='mistral', prompt=prompt)\n",
    "print(\"Summary:\", result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "171450c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a vibrant emerald green garden, butterflies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun was setting, casting a beautiful coral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She wore a stunning royal blue dress that shim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The ocean waves crashed against the shore, lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His eyes were as deep and mysterious as a sapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A rainbow arched over the city, painting the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The magenta blossoms of the bougainvillea stoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The lush grass was a bright verdant green unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The purple hue of the lavender field stretched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The crimson sunset painted the sky with fiery ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  In a vibrant emerald green garden, butterflies...\n",
       "1  The sun was setting, casting a beautiful coral...\n",
       "2  She wore a stunning royal blue dress that shim...\n",
       "3  The ocean waves crashed against the shore, lea...\n",
       "4  His eyes were as deep and mysterious as a sapp...\n",
       "5  A rainbow arched over the city, painting the b...\n",
       "6  The magenta blossoms of the bougainvillea stoo...\n",
       "7  The lush grass was a bright verdant green unde...\n",
       "8  The purple hue of the lavender field stretched...\n",
       "9  The crimson sunset painted the sky with fiery ..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.generate(model='mistral', \n",
    "                           prompt='Generate a list of 10 string elements with random sentences subtly containing the name of a color somewhere',\n",
    "                           options={'seed': 1})\n",
    "sentences = pd.Series(response['response'].split('\"'))\n",
    "sentences = sentences[sentences.str.len()>10]\n",
    "df = sentences.to_frame(name='comment').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e3efa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  \\\n",
      "0  In a vibrant emerald green garden, butterflies...   \n",
      "1  The sun was setting, casting a beautiful coral...   \n",
      "2  She wore a stunning royal blue dress that shim...   \n",
      "3  The ocean waves crashed against the shore, lea...   \n",
      "4  His eyes were as deep and mysterious as a sapp...   \n",
      "5  A rainbow arched over the city, painting the b...   \n",
      "6  The magenta blossoms of the bougainvillea stoo...   \n",
      "7  The lush grass was a bright verdant green unde...   \n",
      "8  The purple hue of the lavender field stretched...   \n",
      "9  The crimson sunset painted the sky with fiery ...   \n",
      "\n",
      "                                               color  \n",
      "0                                            emerald  \n",
      "1                                              coral  \n",
      "2                                               blue  \n",
      "3                                             indigo  \n",
      "4  sapphire (is not a color but i'm assuming you ...  \n",
      "5                                             yellow  \n",
      "6                                            magenta  \n",
      "7                                              green  \n",
      "8                                             purple  \n",
      "9                                                red  \n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"comment\": [\n",
    "#         \"Why is the bike pink.\",\n",
    "#         \"The color of the phone box is red.\",\n",
    "#         \"I really like green cars.\",\n",
    "#         \"Her dress was bright yellow.\",\n",
    "#         \"The wall is painted white.\"\n",
    "#     ]\n",
    "# })\n",
    "# create a new empty column for the LLM result\n",
    "df[\"color\"] = None\n",
    "# creat a standard global prompt\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Extract the color mentioned in the text. \"\n",
    "    \"Reply with ONLY the color word in lowercase (e.g., blue, red). \"\n",
    "    \"If no color is mentioned, reply with 'none'.\"\n",
    ")\n",
    "\n",
    "# loop though comments and prompt the model\n",
    "for i, comment in df[\"comment\"].items():\n",
    "    # prompt = f\"{SYSTEM_PROMPT}\\n\\nText: {comment}\"\n",
    "    \n",
    "    resp = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                    {'role': 'user', 'content': comment}],\n",
    "        stream=False,\n",
    "        options={\n",
    "          # \"temperature\": 0.1, # introduce randomness\n",
    "          \"seed\": 123, # set seed to be able to replicate results\n",
    "          },\n",
    "    )\n",
    "    color = resp[\"message\"][\"content\"].strip().lower()\n",
    "    df.at[i, \"color\"] = color # insert te answer ine the new col\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
